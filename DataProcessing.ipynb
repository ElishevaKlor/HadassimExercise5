{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca443ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "הקובץ נשמר בהצלחה logs.txt\n"
     ]
    }
   ],
   "source": [
    "#סעיף א\n",
    "import pandas as pd\n",
    "\n",
    "#המרה לקובץ טקסט והוספת כותרות לעמודות\n",
    "def add_columns(file_path, txt_path):\n",
    "    \"\"\"פורמט מבוקש עם כותרות TXT לקובץ Excel המרת קובץ\"\"\"\n",
    "    df = pd.read_excel(file_path, header=None, names=['Raw'])\n",
    "    df[['Timestamp', 'Error']] = df['Raw'].str.split(', ', expand=True)\n",
    "    df['Timestamp'] = df['Timestamp'].str.replace('Timestamp: ', '', regex=False)\n",
    "    df['Error'] = df['Error'].str.replace('Error: ', '', regex=False)\n",
    "    df = df[['Timestamp', 'Error']] \n",
    "    df.to_csv(txt_path, index=False, sep=',', header=True)  \n",
    "    print(f\"הקובץ נשמר בהצלחה {txt_path}\")\n",
    "\n",
    "add_columns('logs.txt.xlsx', 'logs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f11281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ה-2 קודי השגיאה השכיחים ביותר: [('WARN_101', 200098), ('ERR_404', 200094)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_error_frequencies(file_path, chunk_size=10000):\n",
    "    \"\"\"ספירת השגיאות בכל chunk\"\"\"\n",
    "    error_counter = Counter()  \n",
    "    for chunk in pd.read_csv(file_path, header=0, names=['Timestamp', 'Error'], chunksize=chunk_size):\n",
    "        error_counter.update(chunk['Error'])  \n",
    "    return error_counter\n",
    "\n",
    "def top_n_error_codes(file_path, n, chunk_size=10000):\n",
    "    \"\"\"מציאת N קודי השגיאה השכיחים ביותר\"\"\"\n",
    "    error_counter = count_error_frequencies(file_path, chunk_size)\n",
    "    return error_counter.most_common(n)  \n",
    "\n",
    "n = 2  \n",
    "top_errors = top_n_error_codes('logs.txt', n)\n",
    "print(f\"ה-{n} קודי השגיאה השכיחים ביותר: {top_errors}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8260556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN_101    200098\n",
      "ERR_404     200094\n",
      "ERR_400     200069\n",
      "INFO_200    199931\n",
      "ERR_500     199808\n",
      "Name: Error, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#בדיקה שזה אכן נכון\n",
    "\n",
    "import pandas as pd\n",
    "def load_data(file_path='logs.txt'):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "df=load_data()\n",
    "print(df['Error'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18af8fb",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right;\">\n",
    "### סיבוכיות זמן\n",
    "\n",
    "### חלוקה ל-2 מקרים:\n",
    "\n",
    "1.\n",
    "### זמן ריצה ממוצע:\n",
    "   - **קריאת שורה מקובץ, חיפוש והוספה ב-Counter.update()**:\n",
    "     - פעולת החיפוש וההוספה במילון פנימי של ה-Counter (המבוסס על hashing table) מתבצעת בזמן O(1) בממוצע.\n",
    "     - לכן, עבור כל פריט בקובץ הלוגים, עדכון ספירת השגיאה הוא פעולה של O(1) בממוצע.\n",
    "     - לכן, הזמן הכולל של עדכון ספירת השגיאות הוא O(M) כש-M הוא מספר השורות בקובץ.\n",
    "   - **מיון השגיאות השכיחות ביותר**:\n",
    "     - לאחר עדכון הספירות, צריך למיין את השגיאות השכיחות ביותר.\n",
    "     - זמן הריצה של מיון הוא O(K log K), כש-K הוא מספר השגיאות השונות (מפתחות המילון).\n",
    "     - K יכול להגיע לסדר גודל של M, אם כל שורה מכילה שגיאה ייחודית ולכן זמן הריצה הוא O(M log M).\n",
    "\n",
    "   ### סיבוכיות זמן ממוצעת:\n",
    "   - **סיבוכיות כוללת**: O(M + M log M) = O(M log M).\n",
    "\n",
    "2.\n",
    " ### זמן ריצה במקרה הגרוע:\n",
    "   - במקרה הגרוע, קונפליקטים ב-hashing עשויים לגרום לפעולה איטית יותר.\n",
    "   - במקרה כזה, זמן הריצה עשוי להגיע ל-O(M) לכל חיפוש או עדכון של שורה.\n",
    "   - לכן, הזמן הכולל של עדכון ספירת השגיאות הוא O(M * M).\n",
    "   - **מיון השגיאות השכיחות ביותר**: שלב המיון נשאר זהה, O(M log M).\n",
    "\n",
    "   ### סיבוכיות זמן במקרה הגרוע:\n",
    "   - **סיבוכיות כוללת**: O(M^2 + M log M) = O(M^2).\n",
    "\n",
    "## סיכום:\n",
    "- **ממוצע**: סיבוכיות הזמן היא O(M + M log M).\n",
    "- **מקרה גרוע**: סיבוכיות הזמן היא O(M * M + M log M).\n",
    "\n",
    "## סיבוכיות מקום:\n",
    "- **O(M)** במקרה הגרוע.\n",
    "- **הסבר**: ספירת השגיאות מתבצעת באמצעות Counter, שהוא בעצם מילון (Dictionary). כל \"קוד שגיאה\" שנמצא יתווסף כ-מפתח במילון, ומספר השגיאות שהוא נמצא בהן יתעדכן כערך.\n",
    "- **במקרה הגרוע**: כל שורה בקובץ יכולה להכיל קוד שגיאה שונה, כלומר מספר השגיאות השונות (ה-keys במילון) עשוי להיות שווה למספר השורות M.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10662d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#סעיף ב\n",
    "#חלק 1 -ביצוע בדיקות\n",
    "def is_parquet_file(file_path):\n",
    "    return file_path.lower().endswith('.parquet')\n",
    "\n",
    "def load_data(file_path):\n",
    "    if is_parquet_file(file_path):\n",
    "        df=pd.read_parquet(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        df=pd.read_excel(file_path)\n",
    "        return df\n",
    "df=load_data('time_series.parquet')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d30146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "סטטיסטיקות בסיסיות:\n",
      "               value\n",
      "count  995566.000000\n",
      "mean       49.962300\n",
      "std        26.675291\n",
      "min         0.000000\n",
      "25%        29.200000\n",
      "50%        49.962300\n",
      "75%        70.700000\n",
      "max       100.000000\n",
      "\n",
      "אורך הדטה: 995566\n",
      "\n",
      "שורות ראשונות:\n",
      "             timestamp  value\n",
      "0  2025-06-28 12:00:52   18.5\n",
      "1  2025-06-01 04:17:23   46.3\n",
      "2  2025-06-10 17:02:57   76.0\n",
      "3  2025-06-23 05:23:22   56.4\n",
      "4  2025-06-05 07:20:08   67.9\n",
      "\n",
      "סוגי נתונים:\n",
      "timestamp     object\n",
      "value        float64\n",
      "dtype: object\n",
      "\n",
      "ערכים חסרים:\n",
      "timestamp    0\n",
      "value        0\n",
      "dtype: int64\n",
      "\n",
      "שמות עמודות:\n",
      "Index(['timestamp', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#חקירה ראשונית של הנתונים\n",
    "def initial_exploration(df):\n",
    "    print(\"סטטיסטיקות בסיסיות:\")\n",
    "    print(df.describe())\n",
    "    print(\"\\nאורך הדטה:\", len(df))\n",
    "    print(\"\\nשורות ראשונות:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nסוגי נתונים:\")\n",
    "    print(df.dtypes)\n",
    "    print(\"\\nערכים חסרים:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nשמות עמודות:\")\n",
    "    print(df.columns)\n",
    "initial_exploration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dbda84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# מסקנות מהחקירה:\n",
    "# העמודה timestamp בפורמט תקין ואין לה ערכים חסרים\n",
    "# יש לבצע בעמודה Value טיפול בערכים חסרים\n",
    "# עמודה Value \n",
    "# אינה מטיפוס מספר ולכן המסקנה היא שנכנסו נתונים שגויים\n",
    "# (top 2025-06-26 03:00:40  not_a_number)\n",
    "# ויש להסירם"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40953cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "העמודה 'timestamp' אינה בפורמט תאריך תקני. מבצע המרה...\n",
      "לא תקינים הומרו ל-NaT.\n",
      "\n",
      "ערכים חסרים:\n",
      "timestamp    0\n",
      "value        0\n",
      "dtype: int64\n",
      "\n",
      "מספר כפילויות: 0\n",
      "ערכים לא מספריים ייחודיים בעמודה 'value':\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# פונקציה לבדוק את סוגי הנתונים בעמודות\n",
    "def check_data_types(df):\n",
    "    print(\"\\nסוגי נתונים:\")\n",
    "    print(df.dtypes)\n",
    "\n",
    "# פונקציה לבדוק ולתקן פורמט תאריך בעמודה 'timestamp'\n",
    "def check_and_fix_timestamp(df):\n",
    "    if pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "        print(\"\\nהעמודה 'timestamp' בפורמט תאריך תקני.\")\n",
    "    else:\n",
    "        print(\"\\nהעמודה 'timestamp' אינה בפורמט תאריך תקני. מבצע המרה...\")\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        print(\"לא תקינים הומרו ל-NaT.\")\n",
    "    return df\n",
    "\n",
    "# פונקציה לבדוק ערכים חסרים בעמודות\n",
    "def check_missing_values(df):\n",
    "    missing_values = df.isnull().sum()\n",
    "    print(\"\\nערכים חסרים:\")\n",
    "    print(missing_values)\n",
    "    return missing_values\n",
    "\n",
    "# פונקציה לבדוק כפילויות ולמחוק אותן\n",
    "def check_and_remove_duplicates(df):\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nמספר כפילויות: {duplicates}\")\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "\n",
    "#בודקת את הערכים הלא-מספריים בעמודה value\n",
    "def check_non_numeric_values(df):\n",
    "    non_numeric_values = df[~df['value'].apply(pd.to_numeric, errors='coerce').notna()]\n",
    "    unique_non_numeric_values = non_numeric_values['value'].unique()\n",
    "    print(\"ערכים לא מספריים ייחודיים בעמודה 'value':\")\n",
    "    print(unique_non_numeric_values)\n",
    "    \n",
    "#הפונקציה מחזירה את ה-DataFrame המעודכן, שבו הערכים הלא-מספריים בעמודה value הומרו ל-NaN\n",
    "def handle_non_numeric_in_value(df):\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    return df\n",
    "\n",
    "# פונקציה למלא ערכים חסרים בעמודות עם ערכים לא מספריים או מספריים\n",
    "def fill_missing_values(df):\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object': \n",
    "            df[column] = df[column].fillna(df[column].mode()[0])  \n",
    "        else:  # אם מדובר בעמודה מספרית\n",
    "            if pd.api.types.is_numeric_dtype(df[column]): \n",
    "                if df[column].skew() > 1: \n",
    "                    df[column] = df[column].fillna(df[column].median())  \n",
    "                else:\n",
    "                    df[column] = df[column].fillna(df[column].mean())  \n",
    "    return df\n",
    "\n",
    "# פונקציה ראשית שמבצעת את כל הבדיקות והטיפולים\n",
    "def process_data(df):\n",
    "    df = check_and_fix_timestamp(df)\n",
    "    check_missing_values(df)\n",
    "    df = check_and_remove_duplicates(df)\n",
    "    check_non_numeric_values(df)\n",
    "    df = handle_non_numeric_in_value(df)\n",
    "    df = fill_missing_values(df)\n",
    "    return df\n",
    " \n",
    "df = process_data(df)  \n",
    "df.to_csv(\"processed_data.csv\", index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38f9e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "סטטיסטיקות בסיסיות:\n",
      "               value\n",
      "count  995566.000000\n",
      "mean       49.962300\n",
      "std        26.675291\n",
      "min         0.000000\n",
      "25%        29.200000\n",
      "50%        49.962300\n",
      "75%        70.700000\n",
      "max       100.000000\n",
      "\n",
      "אורך הדטה: 995566\n",
      "\n",
      "שורות ראשונות:\n",
      "             timestamp  value\n",
      "0  2025-06-28 12:00:52   18.5\n",
      "1  2025-06-01 04:17:23   46.3\n",
      "2  2025-06-10 17:02:57   76.0\n",
      "3  2025-06-23 05:23:22   56.4\n",
      "4  2025-06-05 07:20:08   67.9\n",
      "\n",
      "סוגי נתונים:\n",
      "timestamp     object\n",
      "value        float64\n",
      "dtype: object\n",
      "\n",
      "ערכים חסרים:\n",
      "timestamp    0\n",
      "value        0\n",
      "dtype: int64\n",
      "\n",
      "שמות עמודות:\n",
      "Index(['timestamp', 'value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#בדיקת הקובץ החדש אם הכל תקין עכשיו\n",
    "df=pd.read_csv('processed_data.csv')\n",
    "initial_exploration(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3162cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "נתונים ליום: 2025-06-01\n",
      "נתונים ליום: 2025-06-02\n",
      "נתונים ליום: 2025-06-03\n",
      "נתונים ליום: 2025-06-04\n",
      "נתונים ליום: 2025-06-05\n",
      "נתונים ליום: 2025-06-06\n",
      "נתונים ליום: 2025-06-07\n",
      "נתונים ליום: 2025-06-08\n",
      "נתונים ליום: 2025-06-09\n",
      "נתונים ליום: 2025-06-10\n",
      "נתונים ליום: 2025-06-11\n",
      "נתונים ליום: 2025-06-12\n",
      "נתונים ליום: 2025-06-13\n",
      "נתונים ליום: 2025-06-14\n",
      "נתונים ליום: 2025-06-15\n",
      "נתונים ליום: 2025-06-16\n",
      "נתונים ליום: 2025-06-17\n",
      "נתונים ליום: 2025-06-18\n",
      "נתונים ליום: 2025-06-19\n",
      "נתונים ליום: 2025-06-20\n",
      "נתונים ליום: 2025-06-21\n",
      "נתונים ליום: 2025-06-22\n",
      "נתונים ליום: 2025-06-23\n",
      "נתונים ליום: 2025-06-24\n",
      "נתונים ליום: 2025-06-25\n",
      "נתונים ליום: 2025-06-26\n",
      "נתונים ליום: 2025-06-27\n",
      "נתונים ליום: 2025-06-28\n",
      "נתונים ליום: 2025-06-29\n",
      "נתונים ליום: 2025-06-30\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "chunk_size = 10000\n",
    "chunks = pd.read_csv('processed_data.csv', chunksize=chunk_size)\n",
    "all_data = pd.DataFrame()\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n",
    "    all_data = pd.concat([all_data, chunk], ignore_index=True)\n",
    "    \n",
    "all_data['date'] = all_data['timestamp'].dt.date\n",
    "df_daily = all_data.groupby('date')\n",
    "\n",
    "output_dir = 'split_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for date, group in df_daily:\n",
    "    print(f\"נתונים ליום: {date}\")\n",
    "    group.to_csv(f\"{output_dir}/time_series_{date}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242511a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "קורא את הקובץ: split_files\\time_series_2025-06-01.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-02.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-03.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-04.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-05.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-06.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-07.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-08.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-09.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-10.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-11.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-12.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-13.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-14.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-15.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-16.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-17.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-18.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-19.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-20.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-21.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-22.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-23.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-24.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-25.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-26.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-27.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-28.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-29.csv\n",
      "קורא את הקובץ: split_files\\time_series_2025-06-30.csv\n",
      "התוצאה נשמרה ב: hourly_averages.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def process_daily_files(input_dir='split_files/', output_path='hourly_averages.csv'):\n",
    "    files = glob.glob(os.path.join(input_dir, 'time_series*.csv'))\n",
    "    all_hourly_averages = []\n",
    "    \n",
    "    for file_path in files:\n",
    "        print(f\"קורא את הקובץ: {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        hourly_avg = calculate_hourly_avg(df)\n",
    "        all_hourly_averages.append(hourly_avg)\n",
    "        \n",
    "    final_df = pd.concat(all_hourly_averages, ignore_index=True)\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\"התוצאה נשמרה ב: {output_path}\")\n",
    "\n",
    "def calculate_hourly_avg(df):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.floor('H')  \n",
    "    hourly_avg = df.groupby('hour')['value'].mean().reset_index()\n",
    "    hourly_avg.columns = ['time_start', 'average']\n",
    "    hourly_avg['time_start'] = hourly_avg['time_start'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return hourly_avg\n",
    "\n",
    "process_daily_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83245db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            time_start    average\n",
      "0  2025-06-01 00:00:00  50.486972\n",
      "1  2025-06-01 01:00:00  49.942680\n",
      "2  2025-06-01 02:00:00  49.536653\n",
      "3  2025-06-01 03:00:00  50.151774\n",
      "4  2025-06-01 04:00:00  48.803489\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 720 entries, 0 to 719\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   time_start  720 non-null    object \n",
      " 1   average     720 non-null    float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 11.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#בדיקה\n",
    "df_new = pd.read_csv(\"hourly_averages.csv\")\n",
    "print(df_new.head())  \n",
    "print(df_new.info())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57396afe",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-family: Arial, sans-serif;\">\n",
    "  <h1>פרק 3</h1>\n",
    "  <hr>\n",
    "  <h2>--- תכנון ---</h2>\n",
    "\n",
    "  <h3>מחלקה RealTimeHourlyAverages ובה:</h3>\n",
    "  <h4>א. `hourly_averages` - מבנה נתונים:</h4>\n",
    "  <b>הסבר:</b> נשתמש במילון שבו כל שעה תישמר עם ממוצע נתוניה:\n",
    "  <ul>\n",
    "    <li><b>מפתח:</b> שעה</li>\n",
    "    <li><b>ערך:</b> אובייקט<b>HourlyStats</b> שיאפשר לעדכן את הממוצע בזמן אמת שמכיל :\n",
    "      <ul>\n",
    "        <li>`sum` – סכום כל הערכים שהתקבלו עד כה עבור השעה.</li>\n",
    "        <li>`count` – מספר הערכים שהתקבלו עד כה עבור השעה.</li>\n",
    "        <li>`add_value()`</li>\n",
    "        <li>`get_average()`</li>\n",
    "      </ul>\n",
    "    </li>\n",
    "  </ul>\n",
    "\n",
    "  <h4>ב. `update_Hourly_Average()`</h4>\n",
    "\n",
    "  <h4>ג. `get_Hourly_Average()`</h4>\n",
    "\n",
    "  <hr>\n",
    "  <h2>--- תהליך העבודה ---</h2>\n",
    "  <ul>\n",
    "    <li><b>קבלת נתון חדש:</b> כל פעם שתקבל נתון חדש (בזמן אמת), תזין אותו לשעה המתאימה.</li>\n",
    "    <li><b>עדכון הממוצע:</b> עבור כל שעה, תעדכן את הסכום והכמות, ואז תחזיר את הממוצע החדש.</li>\n",
    "    <li><b>חישוב הממוצע:</b> הממוצע יחושב בכל פעם שיידרש, פשוט על ידי חלוקה של הסכום בכמות.</li>\n",
    "  </ul>\n",
    "\n",
    "  <hr>\n",
    "  <h2>--- תכנון פונקציות ---</h2>\n",
    "\n",
    "  <h3>HourlyStats:</h3>\n",
    "  <ul>\n",
    "    <li>`add_value(value)`: מוסיף ערך חדש ומעדכן את הסכום והכמות.</li>\n",
    "    <li>`get_average()`: מחשב את הממוצע על ידי חלוקה של הסכום בכמות.</li>\n",
    "  </ul>\n",
    "\n",
    "  <h3>RealTimeHourlyAverages:</h3>\n",
    "  <ul>\n",
    "    <li>`update_Hourly_Average(hour, value)`: מעדכן את הממוצע של השעה על ידי הוספת הערך החדש לאובייקט המתאים בשעה המתאימה.</li>\n",
    "    <li>`get_Hourly_Average(hour)`: מחזיר את הממוצע של השעה.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d248316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ממוצע לשעה 14: 11.25\n",
      "ממוצע לשעה 15: 8.5\n"
     ]
    }
   ],
   "source": [
    "class Stats:\n",
    "    def __init__(self):\n",
    "        self.count = 0\n",
    "        self.total = 0.0\n",
    "\n",
    "    def add(self, value):\n",
    "        self.total += value\n",
    "        self.count += 1\n",
    "\n",
    "    def average(self):\n",
    "        return self.total / self.count if self.count > 0 else 0\n",
    "\n",
    "class HourlyAverages:\n",
    "    def __init__(self):\n",
    "        self.averages = {}\n",
    "\n",
    "    def process(self, hour, value):\n",
    "        if hour not in self.averages:\n",
    "            self.averages[hour] = Stats()\n",
    "        self.averages[hour].add(value)\n",
    "\n",
    "    def get_average(self, hour):\n",
    "        return self.averages.get(hour, Stats()).average()\n",
    "\n",
    "average_calculator = HourlyAverages()\n",
    "\n",
    "average_calculator.process(14, 10.5)\n",
    "average_calculator.process(14, 12.0)\n",
    "average_calculator.process(15, 8.5)\n",
    "\n",
    "print(\"ממוצע לשעה 14:\", average_calculator.get_average(14))\n",
    "print(\"ממוצע לשעה 15:\", average_calculator.get_average(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df8d9d",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; text-align: right; font-family: Arial, sans-serif;\">\n",
    "    <div style=\"text-align: right;\">קובץ Parquet</div>\n",
    "    <p style=\"text-align: right;\"> לקובץ parquet קיימים יתרונות רבים כאשר יש טיפול מקדים בנתונים.</p>\n",
    "    <p style=\"text-align: right;\">קובץ Parquet עוזר בזיהוי בעיות בנתונים מראש, דבר שיכול לחסוך זמן ומשאבים בתהליך עיבוד הנתונים.</p>\n",
    "    <hr style=\"width: 50%; margin-right: auto; margin-left: 0;\">\n",
    "    <p style=\"text-align: right;\"><b>לדוגמה:</b></p>\n",
    "    <p style=\"text-align: right;\">אם יש בעיות עם נתונים לא נכונים בעמודה, למשל מילה במקום מספר, השמירה לקובץ לא תעבור בהצלחה.</p>\n",
    "    <p style=\"text-align: right;\">זה עוזר לזהות בעיות בנתונים לפני השמירה, ומונע טעויות שעשויות להתרחש בשלב מאוחר יותר בתהליך.</p>\n",
    "    <p style=\"text-align: right;\">ולכן אין צורך לבצע בדיקות מסוג זה על קובץ Parquet בטעינתו, כי כל הבעיות כבר מזוהות מראש.</p>\n",
    "    <hr style=\"width: 50%; margin-right: auto; margin-left: 0;\">\n",
    "    <p style=\"text-align: right;\">יתרון נוסף וחשוב של קובץ Parquet הוא שבגלל שהוא מבוסס על מבנה נתונים קולומנרי, הוא אופטימלי למקרים של קריאה וכתיבה של נתונים באופן סלקטיבי, כלומר, ניתן לקרוא עמודות מסוימות בלבד מבלי לטעון את כל הקובץ.</p>\n",
    "    <p style=\"text-align: right;\">זה מאפשר שיפור משמעותי בזמני טעינה של הנתונים, במיוחד בקבצים גדולים מאוד.</p>\n",
    "    <p style=\"text-align: right;\">בנוסף, קובץ Parquet משתמש בשיטות דחיסה מתקדמות, שמקטינות את הגודל של הקובץ ומייעלות את השימוש בזיכרון ובזמן המעבד, מה שגורם להאצת תהליך הקריאה והכתיבה של נתונים.</p>\n",
    "    <p style=\"text-align: right;\">כמו כן, מכיוון שקובץ Parquet הוא פורמט פתוח, הוא נתמך על ידי הרבה מערכות ו- frameworks שונים כמו Apache Spark, Apache Hive ו-Pandas, מה שמבטיח תאימות גבוהה עם כלים רבים בתחום ניתוח הנתונים.</p>\n",
    "    <p style=\"text-align: right;\">יתרון זה מאוד מועיל במקרים שבהם רוצים לוודא שהנתונים נשמרים בצורה נכונה ויעילה, לפני שממשיכים את העבודה עליהם או מבצעים אנליזות נוספות.</p>\n",
    "    <br>\n",
    "    <p style=\"text-align: right;\">קובץ Parquet יעבוד על הקוד. השינוי היחיד שצריך לעשות הוא התאמה לטעינת קובץ מסוג Parquet.</p>\n",
    "    <pre style=\"text-align: left; direction: ltr; background-color: #f4f4f4; padding: 10px; border: 1px solid #ddd; border-radius: 4px;\">\n",
    "        <code>\n",
    "def is_parquet_file(file_path):\n",
    "    return file_path.lower().endswith('.parquet')\n",
    "\n",
    "if is_parquet_file('time_series.parquet'):\n",
    "    pd.read_parquet('time_series.parquet', columns=columns)\n",
    "        </code>\n",
    "    </pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696f40e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
